{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5237ca09",
   "metadata": {},
   "source": [
    "# HuggingFace pipelines\n",
    "\n",
    "For this session this Notebook to explore the HuggingFace High Level API, pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bbe7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U \"transformers>=4.41\" \"diffusers\" \"datasets\" \"soundfile\" \"torch\" \"huggingface_hub\" \"python-dotenv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c5884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & environment\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    from IPython.display import Audio, display\n",
    "except ImportError:\n",
    "    Audio = None  # type: ignore\n",
    "    display = print  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f05fd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Notebook path – __file__ is undefined in Jupyter\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "load_dotenv(NOTEBOOK_DIR / \".env\")  # pulls HF_TOKEN into env\n",
    "\n",
    "hf_token: str | None = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(hf_token, add_to_git_credential=True, write_permission=False)\n",
    "else:\n",
    "    print(\"⚠️  No HF_TOKEN found – running in anonymous mode (public models only).\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0079bcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996113181114197}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "sentiment = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=0 if DEVICE == \"cuda\" else -1)\n",
    "sentiment(\"Absolutely thrilled to be diving headfirst into the world of LLMs — mastery, here I come!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9cfdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "C:\\Users\\Coelh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9992453),\n",
       "  'word': 'Barack Obama',\n",
       "  'start': 0,\n",
       "  'end': 12},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9990786),\n",
       "  'word': 'United States',\n",
       "  'start': 55,\n",
       "  'end': 68}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Named‑Entity Recognition\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True, device=0 if DEVICE == \"cuda\" else -1)\n",
    "ner(\"Barack Obama made history as the 44th president of the United States, ushering in a new era of leadership and hope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b89ae3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9889456033706665, 'start': 0, 'end': 12, 'answer': 'Barack Obama'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question Answering\n",
    "\n",
    "qa = pipeline(\"question-answering\", device=0 if DEVICE == \"cuda\" else -1)\n",
    "qa(question=\"Who was the 44th president of the United States?\", context=\"Barack Obama was the 44th president of the United States.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa5fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hugging Face Transformers library is a game-changer in natural language processing . From text classification to question answering and named entity recognition, it powers it all with ease . Beloved by the open-source community, it’s'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarization\n",
    "\n",
    "summariser = pipeline(\"summarization\", device=0 if DEVICE == \"cuda\" else -1)\n",
    "long_text = (\n",
    "    \"The Hugging Face Transformers library is a game-changer in natural language processing! From text classification \"\n",
    "    \"to question answering and named entity recognition, it powers it all with ease. Beloved by the open-source community, \"\n",
    "    \"it’s breaking down barriers and fast-tracking innovation like never before.\"\n",
    ")\n",
    "summariser(long_text, max_length=50, min_length=25, do_sample=False)[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086c8fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Les Data Scientists ont été vraiment étonnés par la puissance et la simplicité de l'API du pipeline HuggingFace.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translation (EN→FR and EN→ES)\n",
    "\n",
    "msg = \"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\"\n",
    "\n",
    "fr = pipeline(\"translation_en_to_fr\", device=0 if DEVICE == \"cuda\" else -1)\n",
    "fr(msg)[0][\"translation_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89f3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': \"Hugging Face's Transformers library is amazing!\",\n",
       " 'labels': ['technology', 'sports', 'politics'],\n",
       " 'scores': [0.9493837952613831, 0.032250262796878815, 0.01836598850786686]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero‑Shot Classification \n",
    "\n",
    "zsc = pipeline(\"zero-shot-classification\", device=0 if DEVICE == \"cuda\" else -1)\n",
    "zsc(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac263ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"If there's one thing I want you to remember about using HuggingFace pipelines, it's that they're the simplest and simplest way to make the data look clean.\\n\\nSo here's my idea: try to make your image look clean by using the same filters we use for our images. Here are our filter settings:\\n\\nFilter: #fetch-filter-from-image #make-image #filter-type: image image_type = 'text/svg';\\n\\nFilter: #filter-filter-from-image #filter-type: image image_type = 'text/css';\\n\\nFilter: #filter-filter-from-image #filter-type: image image_type = 'text/css';\\n\\nFilter: #filter-filter-from-image #filter-type: image image_type = 'text/css';\\n\\nFilter: #filter-filter-from-image #filter-type: image image_type = 'text/css';\\n\\nFilter: #filter-filter-from-image #filter-type: image image_type = 'text/css';\\n\\nFilter: #filter-filter-from-image #filter-type: image image_type = 'text/css';\\n\\nFilter: #filter-filter-from-image #filter\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Generation\n",
    "\n",
    "gen = pipeline(\"text-generation\", device=0 if DEVICE == \"cuda\" else -1)\n",
    "gen(\"If there's one thing I want you to remember about using HuggingFace pipelines, it's\", max_length=60)[0][\"generated_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
